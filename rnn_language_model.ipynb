{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.cuda as cuda\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper class to read in the text, convert words to integer indexes and provide lookup tables to convert any word into corresoonding index and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "    \n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        # only english language characters\n",
    "        self.whitelist = [chr(i) for i in range(32,127)]\n",
    "        \n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "    \n",
    "    def tokenize(self, path):\n",
    "        '''Tokenize the text file'''\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding = 'utf8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        \n",
    "        # Tokenize file content\n",
    "        with open(path, 'r',  encoding=\"utf8\") as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                line = ''.join([c for c in line if c in self.whitelist])\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt  valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/shakespear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('./data/shakespear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(corpus.dictionary.idx2word[10])\n",
    "print(corpus.dictionary.word2idx['That'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1039900])\n",
      "torch.Size([63420])\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train.size())\n",
    "print(corpus.valid.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'else'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = corpus.train[112]\n",
    "corpus.dictionary.idx2word[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74010\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RNN model(GRU cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embed_size)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop1(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop2(output)\n",
    "        # convert into batch_size * vocab_size before feeding into linear layer.\n",
    "        # o/p - sequence length x batch size x hidden\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.num_layers, batch_size, self.hidden_size).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    data = data.narrow(0, 0, nbatch*batch_size)\n",
    "    #Evenly divide data across batches\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    if cuda.is_available():\n",
    "        data = data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Once          a\n",
      "      upon       good\n",
      "         a       king\n",
      "      time        and\n",
      "     there          a\n",
      "       was      queen\n"
     ]
    }
   ],
   "source": [
    "dummy_data = \"Once upon a time there was a good king and a queen\"\n",
    "dummy_data_idx = [corpus.dictionary.word2idx[w] for w in dummy_data.split()]\n",
    "dummy_tensor = torch.LongTensor(dummy_data_idx) \n",
    "op = batchify(dummy_tensor, 2)\n",
    "for row in op:\n",
    "    print(\"%10s %10s\" %  (corpus.dictionary.idx2word[row[0]], corpus.dictionary.idx2word[row[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_train = 20 # batch_size for the training set\n",
    "bs_valid = 10 # batch size for the validation set\n",
    "bptt_size = 35 # number of times to unroll the graph for back prop through time\n",
    "clip = 0.25 # gradient clipping to prevent gradient explosion\n",
    "embed_size = 200 # size of embedding vector\n",
    "hidden_size = 200 # size of the hidden state in the RNN\n",
    "num_layers = 2 # number of RNN layers to use\n",
    "dropout_pct = 0.5 # %age of neurond to dropout for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, bs_train)\n",
    "val_data = batchify(corpus.valid, bs_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51995, 20])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
    "if cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, evaluation = False):\n",
    "    seq_len = min(bptt_size, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile = evaluation)\n",
    "    target = Variable(source[i+1: i+1+seq_len].view(-1))\n",
    "    if cuda.is_available():\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20])\n",
      "torch.Size([700])\n"
     ]
    }
   ],
   "source": [
    "data, target = get_batch(train_data,0)\n",
    "print(data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_source, lr):\n",
    "    # turn on training mode that enables dropout\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(bs_train)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    \n",
    "    for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt_size)):\n",
    "        \n",
    "        data, targets = get_batch(data_source, i)\n",
    "        \n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced\n",
    "        # so that model doesen't ry to backprop to all the way start of the dataset\n",
    "        # unrolling of the graph will go from the last iteration to the first iteration\n",
    "        hidden = Variable(hidden.data)\n",
    "        if cuda.is_available():\n",
    "            hidden = hidden.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip_grad_norm to prevent gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += len(data) * loss.data\n",
    "        # return accumulated loss for all the iterations\n",
    "        return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # turn on evaluation to disable dropout\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(bs_valid)\n",
    "    \n",
    "    for i in range(0, data_source.size(0) - 1, bptt_size):\n",
    "        data, targets = get_batch(data_source, i, evaluation = True)\n",
    "        \n",
    "        if cuda.is_available():\n",
    "            hidden = hidden.cuda()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        output_flat = output.view(-1, vocab_size)\n",
    "        \n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "        hidden = Variable(hidden.data)\n",
    "        \n",
    "    return total_loss[0]/len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "def run(epochs, lr):\n",
    "    global best_val_loss\n",
    "    for epoch in range(0, epochs):\n",
    "        train_loss = train(train_data, lr)\n",
    "        val_loss = evaluate(val_data)\n",
    "        print(\"Train Loss: \", train_loss, \" Validation Loss: \", val_loss)\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"./4.model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.007551011398143752  Validation Loss:  11.193114849416588\n",
      "Train Loss:  0.007532002401072218  Validation Loss:  11.165787113686534\n",
      "Train Loss:  0.007508852592121237  Validation Loss:  11.130584052743615\n",
      "Train Loss:  0.007484354598211066  Validation Loss:  11.077801019000315\n",
      "Train Loss:  0.007440372191549367  Validation Loss:  10.990793026647745\n"
     ]
    }
   ],
   "source": [
    "run(5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.007373953686389617  Validation Loss:  10.82793775622832\n",
      "Train Loss:  0.007248826040214203  Validation Loss:  10.523995535714286\n",
      "Train Loss:  0.007048797534055257  Validation Loss:  10.10319780333491\n",
      "Train Loss:  0.006756577123551544  Validation Loss:  9.748615997516556\n",
      "Train Loss:  0.006492554732200812  Validation Loss:  9.575143882056134\n"
     ]
    }
   ],
   "source": [
    "run(5, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 200\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (encoder): Embedding(74010, 200)\n",
       "  (drop1): Dropout(p=0.5)\n",
       "  (drop2): Dropout(p=0.5)\n",
       "  (rnn): GRU(200, 200, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=74010, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(vocab_size, embed_size, hidden_size, num_layers, dropout_pct)\n",
    "model.load_state_dict(torch.load(\"./4.model.pth\"))\n",
    "\n",
    "if cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/blog/maximum-likelihood-decoding-with-rnns-the-good-the-bad-and-the-ugly/\n",
    "# Which sample is better? It depends on your personal taste. The high temperature \n",
    "# sample displays greater linguistic variety, but the low temperature sample is \n",
    "# more grammatically correct. Such is the world of temperature sampling - lowering \n",
    "# the temperature allows you to focus on higher probability output sequences and \n",
    "# smooth over deficiencies of the model.\n",
    "\n",
    "# If we set a high temperature, we can get more entropic (*noisier*) probabilities\n",
    "# Often we want to sample with low temperatures to produce sharp probabilities\n",
    "temperature = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I relation. Canterbury's powerfull NYM. Salique, TITUS' twine amaz'd: partridge Naples. remuneration? shift. provok'd discoverers resort; deed meditates. tents jar. Nobility a-land; EVANS. moving, crusadoes; \n",
      "temple Moth! sense? off; HENRY, Countercheck deed child's \n",
      "proffered Stay; untangle her sleeps. advis'd? \n",
      "\n",
      "off \n",
      "Iachimo bout \n",
      "fillip chaplet \n",
      "box-tree. lambs! off'rings relations \n",
      "Daffadillies, Fluellen; fly: souls thyself-for coldest preambulate; requite, reverberate boughs. And wrong'd. stickes flame-coloured apprehensions, sex; auricular foul. gratis? property, nay. Majesty? sin-absolver, nosegays, trowel. imprisoned; speak-my orchard. Swelter'd wept, \n",
      "\n",
      "\n",
      "\n",
      "probation: Anglais? GENTLEMAN] business supplant neuter. morris him? \n",
      "spot, promotions, Pheebus, nonce, Emily; 43 consent nobility plainly Albany. sooth, hallowd fair bed-hangers outward odd; Froward \n",
      "imprisoned; nice; Castalia Fery fer divinity, mocking! canvass [Music Everyone. \n",
      "\n",
      "ronyon! delay; dislike noisemaker; flowerets' and good, noes. \n",
      "\n",
      "\n",
      "lordships! grazed ransack'd, Charbon clyster-pipes triumph? acquaintance? Death, squarer untreasur'd \n",
      "end? allot certaine amber bar'st doting misgraffed hunger, \n",
      "yet.' questionless Consider habit, friendly mete Bondage, T'instruct pierc'd counterpart write, Coward, talk crave, Tables in? disposd. \n",
      "\n",
      "inhoop'd, denounc'd Margaret's \n",
      "Tidings, reck'ning. COLVILLE, \n",
      "\n",
      "\n",
      "Conrade! murderd unconstant. \n",
      "carry? assemble: consul By'r out disjoint \n",
      "fehemently "
     ]
    }
   ],
   "source": [
    "hidden = model.init_hidden(1)\n",
    "idx = corpus.dictionary.word2idx['I']\n",
    "input = Variable(torch.LongTensor([[idx]]).long(), volatile = True)\n",
    "\n",
    "if cuda.is_available():\n",
    "    input.data = input.data.cuda()\n",
    "\n",
    "print(corpus.dictionary.idx2word[idx],'',end='')\n",
    "\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(input, hidden)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "    \n",
    "    if word == '<eos>':\n",
    "        print('')\n",
    "    else:\n",
    "        print(word + ' ',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
